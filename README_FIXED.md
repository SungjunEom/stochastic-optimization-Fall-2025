# ğŸ”§ Noisy Observation ë¬¸ì œ í•´ê²° ì™„ë£Œ

## ğŸ“Œ ë¬¸ì œ ìš”ì•½

**ì›ë˜ ë¬¸ì œ**: Built-in solverì™€ í™•ë¥ ì  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë“¤ ì‚¬ì´ì— **ì„±ëŠ¥ ì°¨ì´ê°€ ì—†ì—ˆìŒ**

**ì›ì¸**: 
- Noisy observationì´ ì•„ë‹ˆë¼ **noisy target**ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ìˆì—ˆìŒ
- ë…¸ì´ì¦ˆê°€ ìµœì í™” ì „ì— ë‹¨ í•œ ë²ˆë§Œ ì¶”ê°€ë˜ì–´, ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì´ ë™ì¼í•œ deterministic problemì„ í’€ê³  ìˆì—ˆìŒ
- Gradient-based ë°©ë²•ì´ noisy gradientë¥¼ ê²½í—˜í•˜ì§€ ëª»í•¨

## âœ… í•´ê²° ë°©ë²•

### 1. Gradient-based Optimizer êµ¬í˜„

Built-in solverë¥¼ ì œê±°í•˜ê³  ì§ì ‘ êµ¬í˜„:

**íŒŒì¼ ì¶”ê°€**:
- `src/optimizers/gradient_descent_ik.m` - Numerical gradient + momentum
- `src/optimizers/adam_ik.m` - Adam optimizer (adaptive learning rate)

**ì™œ ì§ì ‘ êµ¬í˜„?**
- MATLABì˜ `inverseKinematics`ëŠ” custom loss functionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
- Noisy observationì„ ì œëŒ€ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ loss evaluationë§ˆë‹¤ ë…¸ì´ì¦ˆ í•„ìš”
- Gradient-based ë°©ë²•ì´ noisy gradientì—ì„œ ì–´ë–»ê²Œ ê³ ìƒí•˜ëŠ”ì§€ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ

### 2. Noisy Loss Function

**íŒŒì¼**: `src/utils/ik_loss_noisy.m`

```matlab
function loss = ik_loss_noisy(theta, x_d, noise_level)
    % ë§¤ evaluationë§ˆë‹¤ ì„¼ì„œ ë…¸ì´ì¦ˆ ì¶”ê°€
    x_current_clean = franka_forward_kinematics(theta);
    x_current_observed = x_current_clean + noise_level * randn(6,1);
    
    pose_error = norm(x_current_observed - x_d)^2;
    loss = pose_error;
end
```

**í•µì‹¬**: 
- ë™ì¼í•œ Î¸ì— ëŒ€í•´ì„œë„ ë§¤ë²ˆ ë‹¤ë¥¸ loss ë°˜í™˜
- ì§„ì§œ noisy observation í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜

### 3. ëª¨ë“  Optimizer ìˆ˜ì •

**ìˆ˜ì •ëœ íŒŒì¼**:
- `src/optimizers/simple_random_search.m`
- `src/optimizers/localized_random_search.m`
- `src/optimizers/enhanced_localized_random_search.m`

**ë³€ê²½ì‚¬í•­**:
```matlab
% ì´ì „
L_k = ik_loss(theta_hat, x_d);

% í˜„ì¬
L_k = ik_loss_noisy(theta_hat, x_d, noise_level);
```

### 4. ì‹œë‚˜ë¦¬ì˜¤ ì¬ì„¤ê³„

**test.m**:

#### ì‹œë‚˜ë¦¬ì˜¤ 0: Deterministic (ê¸°ì¤€ì„ )
- ë…¸ì´ì¦ˆ ì—†ìŒ
- ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ ì„±ëŠ¥ ì¸¡ì •

#### ì‹œë‚˜ë¦¬ì˜¤ 1: Noisy Observation â­ **í•µì‹¬**
- **ë§¤ iterationë§ˆë‹¤** FK ê´€ì¸¡ ì‹œ ë…¸ì´ì¦ˆ ì¶”ê°€
- Gradient-based: Noisy gradient ê³„ì‚° â†’ **ë¶ˆì•ˆì •**
- Stochastic: ì—¬ëŸ¬ ìƒ˜í”Œì˜ í‰ê·  íš¨ê³¼ â†’ **Robust**

#### ì‹œë‚˜ë¦¬ì˜¤ 2: FK ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
- ìµœì í™” ê³¼ì •ì—ì„œ FK ëª¨ë¸ì— ì˜¤ì°¨
- ì‹œë‚˜ë¦¬ì˜¤ 1ê³¼ ë™ì¼í•œ íš¨ê³¼ (noisy observation)

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ì¡°ì¸íŠ¸ ì œì–´ ì˜¤ì°¨
- ìµœì í™” **í›„** ë¡œë´‡ì´ ëª…ë ¹ì„ ì •í™•íˆ ë”°ë¼ê°€ì§€ ëª»í•¨
- ìµœì í™” ê³¼ì •ì—ëŠ” ì˜í–¥ ì—†ìŒ (ê³µì •í•œ ë¹„êµ)

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

### Deterministic (ì‹œë‚˜ë¦¬ì˜¤ 0)
```
âœ… ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë¹„ìŠ·í•œ ì„±ëŠ¥
```

### Noisy Observation (ì‹œë‚˜ë¦¬ì˜¤ 1, 2) âš¡
```
âŒ Gradient Descent: ë†’ì€ ì˜¤ì°¨
   - Noisy gradientë¡œ ì¸í•œ ë¶ˆì•ˆì •í•œ ì—…ë°ì´íŠ¸
   - ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ ì´ë™ ê°€ëŠ¥
   
âš ï¸ Adam: ì¤‘ê°„ ì˜¤ì°¨
   - Adaptive learning rateë¡œ ì–´ëŠ ì •ë„ ì™„í™”
   - ì—¬ì „íˆ noisy gradient ë¬¸ì œ ì¡´ì¬

âœ… SRS, LRS, ELRS: ë‚®ì€ ì˜¤ì°¨
   - Noisy lossë¥¼ ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§
   - í‰ê· ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë°©í–¥ íƒìƒ‰
   - Gradient ê³„ì‚° ë¶ˆí•„ìš”
```

### ì œì–´ ì˜¤ì°¨ (ì‹œë‚˜ë¦¬ì˜¤ 3)
```
âœ… ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë¹„ìŠ·í•œ ì˜í–¥
   - ìµœì í™”ëŠ” ì™„ë£Œëœ ìƒíƒœ
   - ì œì–´ ì˜¤ì°¨ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ë¬´ê´€
```

## ğŸ” í•µì‹¬ ì°¨ì´ì 

| í•­ëª© | ì´ì „ | í˜„ì¬ |
|------|------|------|
| **ë…¸ì´ì¦ˆ ì¶”ê°€** | ìµœì í™” ì „ 1íšŒ | ë§¤ iterationë§ˆë‹¤ |
| **Loss function** | Deterministic | Stochastic |
| **Gradient** | Clean | Noisy |
| **ë¬¸ì œ ì„±ê²©** | Shifted target | True noisy observation |
| **ì•Œê³ ë¦¬ì¦˜ ì°¨ì´** | âŒ ê±°ì˜ ì—†ìŒ | âœ… ëª…í™•íˆ ë“œëŸ¬ë‚¨ |

## ğŸ“Š í…ŒìŠ¤íŠ¸ ì•Œê³ ë¦¬ì¦˜ (5ê°œ)

1. **Gradient Descent** (GD)
   - Numerical gradient (finite difference)
   - Momentum ì‚¬ìš©
   - Learning rate decay
   
2. **Adam Optimizer** (Adam)
   - Adaptive learning rate
   - 1ì°¨, 2ì°¨ ëª¨ë©˜íŠ¸ ì¶”ì •
   - Noisy gradientì— ë” robust

3. **Simple Random Search** (SRS)
   - ì „ì—­ íƒìƒ‰
   - Gradient-free
   
4. **Localized Random Search** (LRS)
   - êµ­ì†Œ íƒìƒ‰
   - Â±direction ì‹œë„
   
5. **Enhanced LRS** (ELRS)
   - Bias termìœ¼ë¡œ íƒìƒ‰ ë°©í–¥ í•™ìŠµ
   - ê°€ì¥ sophisticatedí•œ stochastic method

## ğŸš€ ì‹¤í–‰ ë°©ë²•

```matlab
% MATLABì—ì„œ ì‹¤í–‰
run('test.m')

% ê²°ê³¼ëŠ” 'ik_comparison_results.mat'ì— ì €ì¥ë¨
```

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„

**ì‹œë‚˜ë¦¬ì˜¤ 1ì—ì„œ ê°€ì¥ í° ì°¨ì´ ì˜ˆìƒ**:

```
Gradient Descent: ë†’ì€ loss (noisy gradient ë•Œë¬¸)
â†“
Adam: ì¤‘ê°„ loss (adaptive LRë¡œ ì™„í™”)
â†“  
SRS, LRS, ELRS: ë‚®ì€ loss (gradient-free)
```

## ğŸ“ êµí›ˆ

1. **Noisy observation â‰  Noisy target**
   - ë…¸ì´ì¦ˆê°€ ìµœì í™” ê³¼ì •ì— í¬í•¨ë˜ì–´ì•¼ í•¨
   - ë‹¨ìˆœíˆ targetì„ shiftí•˜ëŠ” ê²ƒì€ ì˜ë¯¸ ì—†ìŒ

2. **Gradient-basedì˜ ì•½ì **
   - Noisy observationì—ì„œ ë§¤ìš° ì·¨ì•½
   - Noisy gradientê°€ wrong directionìœ¼ë¡œ ìœ ë„

3. **Stochastic optimizationì˜ ê°•ì **
   - Gradient ê³„ì‚° ë¶ˆí•„ìš”
   - ì—¬ëŸ¬ ìƒ˜í”Œì˜ í‰ê·  íš¨ê³¼
   - Noisy environmentì—ì„œ robust

4. **ê³µì •í•œ ë¹„êµì˜ ì¤‘ìš”ì„±**
   - Built-in solverëŠ” ë‚´ë¶€ êµ¬í˜„ì„ ì œì–´í•  ìˆ˜ ì—†ìŒ
   - ì§ì ‘ êµ¬í˜„í•´ì•¼ ë™ì¼í•œ ì¡°ê±´ì—ì„œ ë¹„êµ ê°€ëŠ¥

## ğŸ“ ì£¼ìš” íŒŒì¼

### ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼
- `src/optimizers/gradient_descent_ik.m` - GD optimizer
- `src/optimizers/adam_ik.m` - Adam optimizer
- `src/utils/ik_loss_noisy.m` - Noisy loss function
- `CHANGES.md` - ìƒì„¸ ë³€ê²½ ë‚´ì—­
- `README_FIXED.md` - ì´ ë¬¸ì„œ

### ìˆ˜ì •ëœ íŒŒì¼
- `test.m` - Built-in ì œê±°, gradient-based ì¶”ê°€
- `src/optimizers/simple_random_search.m` - Noisy loss ì§€ì›
- `src/optimizers/localized_random_search.m` - Noisy loss ì§€ì›
- `src/optimizers/enhanced_localized_random_search.m` - Noisy loss ì§€ì›

### ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨
- `src/utils/franka_ik_builtin.m` - ì œê±° (ê³µì •í•œ ë¹„êµ ë¶ˆê°€)
- `src/utils/franka_ik_builtin_robust.m` - ì œê±° (ì„ì‹œ í•´ê²°ì±…ì´ì—ˆìŒ)

## ğŸ”¬ Noisy Gradient ì‹œê°í™” (ê°œë…)

```
Deterministic Gradient:
  Î¸ â†’ L(Î¸) â†’ âˆ‡L â†’ clean gradient â†’ Î¸_new âœ…

Noisy Gradient:
  Î¸ â†’ L_noisy(Î¸) â†’ âˆ‡L_noisy â†’ wrong direction â†’ Î¸_new âŒ
  Î¸ â†’ L_noisy(Î¸) â†’ âˆ‡L_noisy â†’ wrong direction â†’ Î¸_new âŒ
  Î¸ â†’ L_noisy(Î¸) â†’ âˆ‡L_noisy â†’ wrong direction â†’ Î¸_new âŒ
  â‡’ í‰ê· ì ìœ¼ë¡œëŠ” ë§ì§€ë§Œ, ë§¤ stepë§ˆë‹¤ ë¶ˆì•ˆì •
```

## ğŸ¯ ê²°ë¡ 

ì´ì œ **ì§„ì§œ noisy observation í™˜ê²½**ì—ì„œ í™•ë¥ ì  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë“¤ì´ gradient-based ë°©ë²•ë³´ë‹¤ ì–¼ë§ˆë‚˜ robustí•œì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

**í•µì‹¬**: Gradient-basedëŠ” noisy gradient ë•Œë¬¸ì— ê³ ìƒí•˜ê³ , gradient-free stochastic methodsëŠ” ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•  ê²ƒì…ë‹ˆë‹¤.

